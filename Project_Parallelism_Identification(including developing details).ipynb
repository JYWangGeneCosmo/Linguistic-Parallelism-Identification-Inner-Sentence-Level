{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/genecosmo/Desktop/Computer Applications in Linguistics \n",
      "\n",
      "['2009-01-20-Barack-Obama.txt', '2017-01-20-Donald-J-Trump.txt', '2001-01-20-George-W-Bush.txt']\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the 3 inaugural addresses in different times \n",
    "# (from the following years: 2001, 2009, 2017) for manual annotation work later.\n",
    "\n",
    "print(os.getcwd(), \"\\n\") # make use of the default path\n",
    "\n",
    "filelist = [fileid for fileid in os.listdir(\n",
    "    \"./American-Inaugural-Address-Corpus\") \n",
    "            if fileid[:4] in [ \"2001\", \"2009\", \"2017\"]]\n",
    "print(filelist) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "# enter the paths of Stanford POS Tagger .jar file as well as the model to be used\n",
    "jar = \"/Users/Shared/stanford-postagger-full-2018-10-16/stanford-postagger-3.9.2.jar\"\n",
    "model = \"/Users/Shared/stanford-postagger-full-2018-10-16/models/english-left3words-distsim.tagger\"\n",
    "# Instantiate an English pos-tagger using the jar and model defined above \n",
    "pos_tagger_en = StanfordPOSTagger(model, jar, encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2009-01-20-Barack-Obama.txt\n",
      "1985-01-20-Ronald-Reagan.txt\n",
      "1977-01-20-Jimmy-Carter.txt\n",
      "2017-01-20-Donald-J-Trump.txt\n",
      "2001-01-20-George-W-Bush.txt\n"
     ]
    }
   ],
   "source": [
    "# Use Stanford POS-tagger defined above to Tag all source texts (English)\n",
    "\n",
    "for fileid in filelist:\n",
    "    if fileid.endswith(\".txt\"):                           # go through all English texts to apply tagger\n",
    "        print(fileid)            \n",
    "        with open (\"American-Inaugural-Address-Corpus/\" + fileid, encoding = \"utf-8\") as f:   \n",
    "            raw = f.read()\n",
    "            tokenized_text = word_tokenize(raw)              # tokenizing\n",
    "            tagged_text = pos_tagger_en.tag(tokenized_text)  # pos-tagging\n",
    "\n",
    "# Write the tagged text into new .txt files in specific format respectively\n",
    "\n",
    "        with open (\"American-Inaugural-Address-Corpus/tagged_\" + fileid, \"w\", encoding = \"utf-8\") as tag_f:\n",
    "            write_text = \"\"\n",
    "            for (a, b) in tagged_text:\n",
    "                write_text += a + \"_\" + b +\" \"   # combine word and tag together in the format: \"My_PRP$ \"\n",
    "            \n",
    "            # add newline character after the character which marks the end of a sentence \n",
    "            result = write_text.replace(\"_. \", \"_.\\n\").replace(\";_:\", \";_:\\n\").replace(\n",
    "            \":_:\", \":_:\\n\")   # \"_.\" include \"!\", \"?\" these sentence closers\n",
    "            \n",
    "                \n",
    "            tag_f.write(result)       # write all the results into corresponding .txt file     \n",
    "            \n",
    "\n",
    "for fileid in filelist:\n",
    "    with open (\"American-Inaugural-Address-Corpus/tagged_\"+ fileid) as f:\n",
    "        sentences = []\n",
    "        for row in f:\n",
    "            sentences.append(row)  \n",
    "        \n",
    "# Get corresponding .xlsx files which contain every sentence of the speech in the first column.\n",
    "        \n",
    "        df_sent = pd.DataFrame({\"sentence\" : sentences[:len(sentences)]})\n",
    "        df_sent.to_excel(\"American-Inaugural-Address-Corpus/Tagged/\"+ \n",
    "                    fileid + \"sent.xlsx\", encoding = \"utf-8\", index = False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get indexes of all coordinators\n",
    "def get_index_of_coor(sent):\n",
    "    split_sent = sent.split()\n",
    "    indexes_of_coor = [-1]  # this ancillary element is used to compute the index of the first coordinator\n",
    "    for w in split_sent:\n",
    "        if w.startswith(\",\") or w.endswith(\"_CC\") or w.lower() == \"while_in\":\n",
    "            # Use the formula to get the right index \n",
    "            # indexes_of_coor[-1] means the last element(index) of the index list\n",
    "            indexes_of_coor.append(split_sent.index(w) + indexes_of_coor[-1] +1 ) \n",
    "           \n",
    "            # chop the previous part of the sentence off (by +1) in order to find the next coordinator\n",
    "            split_sent = split_sent[(split_sent.index(w) +1) :]  \n",
    "            \n",
    "    # Exclude the conjunction at the beginning of the sentence       \n",
    "    if 0 in indexes_of_coor:   \n",
    "        indexes_of_coor = indexes_of_coor[2:]\n",
    "    else:\n",
    "        indexes_of_coor = indexes_of_coor[1:] # First ancillary index should be excluded\n",
    "    return indexes_of_coor  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 11, 22, 25, 26, 36]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_7 = \"While_IN the_DT Congress_NNP determines_VBZ the_DT objects_NNS and_CC the_DT sum_NN of_IN appropriations_NNS ,_, the_DT officials_NNS of_IN the_DT executive_NN departments_NNS are_VBP responsible_JJ for_IN honest_JJ and_CC faithful_JJ disbursement_NN ,_, and_CC it_PRP should_MD be_VB their_PRP$ constant_JJ care_NN to_TO avoid_VB waste_NN and_CC extravagance_NN ._.\"\n",
    "get_index_of_coor(test_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used for comparing 2 phrases as string\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def compare(str1, str2):\n",
    "    return SequenceMatcher(None, str1, str2).ratio() \n",
    "# returns the similarity score (float in [0,1]) between input strings, the higher the score is, more similar 2 Strings are. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_seq (word_tag_sequence):\n",
    "    pos_sequence = ''.join([w[w.index(\"_\"):w.index(\"_\")+3] for w in word_tag_sequence])\n",
    "    return pos_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_seq_raw (word_tag_sequence):\n",
    "    pos_sequence = ''.join([w[w.index(\"_\"):] for w in word_tag_sequence])\n",
    "    return pos_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar(prev, after):\n",
    "    similar_pair = 0\n",
    "    if len(prev) > 2 and len(after) > 2:\n",
    "        \n",
    "        if len(prev) == len(after):\n",
    "            if compare(get_pos_seq(prev), get_pos_seq(after)) > 0.79 or compare(\n",
    "            get_pos_seq(prev[1:]), get_pos_seq(after)) > 0.79 or compare(\n",
    "            get_pos_seq(prev[2:]), get_pos_seq(after)) > 0.79 or compare(\n",
    "            get_pos_seq(prev), get_pos_seq(after[: -1])) > 0.79 or compare(\n",
    "            get_pos_seq(prev), get_pos_seq(after[: -2])) > 0.79:\n",
    "                #print(\"1\")\n",
    "                similar_pair += 1\n",
    "        elif len(prev) - len(after) == 1:   # previous subsentence is longer, so latter subsentence as length-base\n",
    "            if compare(get_pos_seq(prev[-len(after) :]), get_pos_seq(after)) > 0.79 or compare(\n",
    "            get_pos_seq(prev[-len(after)-1:]), get_pos_seq(after)) > 0.79:\n",
    "                #print(\"2\")\n",
    "                similar_pair += 1\n",
    "        elif len(prev) - len(after) >= 2:\n",
    "            if compare(get_pos_seq(prev[-len(after) :]), get_pos_seq(after)) > 0.79 or compare(\n",
    "            get_pos_seq(prev[-len(after)-1:]), get_pos_seq(after)) > 0.79 or compare(\n",
    "            get_pos_seq(prev[-len(after)-2:]), get_pos_seq(after)) > 0.79:\n",
    "                #print(\"3\")\n",
    "                similar_pair += 1\n",
    "        elif len(prev) - len(after) == -1:\n",
    "            if compare(get_pos_seq(prev), get_pos_seq(after[:len(prev)])) > 0.79 or compare(\n",
    "            get_pos_seq(prev), get_pos_seq(after[:len(prev)+1])) > 0.79:\n",
    "                #print(\"4\")\n",
    "                similar_pair += 1\n",
    "        elif len(prev) - len(after) <= -2:\n",
    "            if compare(get_pos_seq(prev), get_pos_seq(after[:len(prev)])) > 0.79 or compare(\n",
    "            get_pos_seq(prev), get_pos_seq(after[:len(prev)+1])) > 0.79 or compare(\n",
    "            get_pos_seq(prev), get_pos_seq(after[:len(prev)+2])) > 0.79:\n",
    "                #print(\"5\")\n",
    "                similar_pair += 1\n",
    "   \n",
    "   # If one of the subsentence is less than 3 words, no normalization of PoS-tags will be performed                 \n",
    "    elif len(prev) == 2 or len(after) == 2: \n",
    "        if len(prev) >= len(after):\n",
    "            if compare(get_pos_seq_raw(prev[-len(after):]), get_pos_seq_raw(after)) == 1:\n",
    "                similar_pair += 1\n",
    "        if len(prev) < len(after):\n",
    "            if compare(get_pos_seq_raw(prev), get_pos_seq_raw(after[:len(prev)])) == 1:\n",
    "                similar_pair += 1\n",
    "                \n",
    "    return similar_pair\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reversed parallelism\n",
    "def reversed_simi(prev, after):\n",
    "    similar_phrase_pair = 0\n",
    "    if len(prev) > 1 and len(after) > 1:\n",
    "        pos = after[0][after[0].index(\"_\"):]\n",
    "        for w in prev[1:]:   # avoid \"index_out_of_range\" problem\n",
    "            if w.endswith(pos):\n",
    "                reverse_start = w\n",
    "                index_reverse_start = prev.index(reverse_start)\n",
    "                w_before = prev[index_reverse_start - 1]\n",
    "                if w_before[w_before.index(\"_\"):] == after[1][after[1].index(\"_\"):]:\n",
    "                    similar_phrase_pair += 1\n",
    "    return similar_phrase_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only extract word-recurrence in particular positions\n",
    "def recurrence_w (prev, after):\n",
    "    recur = 0\n",
    "    if len(prev) >= len(after):\n",
    "        length = len(after)\n",
    "        for index in range(length):\n",
    "            if prev[-length : ][index].lower() == after[index].lower():\n",
    "                recur += 1\n",
    "    if len(prev) < len(after):\n",
    "        length = len(prev)\n",
    "        for index in range(length):\n",
    "            if prev[index].lower() == after [: length][index].lower():\n",
    "                recur += 1\n",
    "    return recur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_of_CC(sent):\n",
    "    sp_sent = sent.split()\n",
    "    counter = 0\n",
    "    for w_t in sp_sent:\n",
    "        if w_t.endswith(\"_CC\"):\n",
    "            counter += 1\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigram_repetition(sent):\n",
    "    num_tri = 0\n",
    "    if len(sent.split()) > 2:\n",
    "        \n",
    "        trigrams = list(nltk.trigrams([w[w.index(\"_\"):] for w in sent.split()]))\n",
    "        if len(trigrams) == 5:\n",
    "            if trigrams[0] == trigrams[4]:\n",
    "                num_tri += 1\n",
    "        if len(trigrams) == 6:\n",
    "            if trigrams[0] == trigrams[4] or trigrams[0] == trigrams[5] or trigrams[1] == trigrams[5]:\n",
    "                num_tri += 1\n",
    "        if len(trigrams) > 6:\n",
    "            for i in range(len(trigrams)-4):\n",
    "                if trigrams[i] == trigrams[i+4]:\n",
    "                    num_tri += 1\n",
    "            for i in range(len(trigrams)-5):\n",
    "                if trigrams[i] == trigrams[i+5]:\n",
    "                    num_tri += 1\n",
    "    return num_tri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_repetition(\"We_PRP treat_VBP it_PRP like_IN a_DT nice-to-have_JJ instead_RB of_IN a_DT must-have_JJ ._.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1:  {'simi': True, 'recur': 1, 'num_CC': 0, 'num_tri': False}\n",
      "t2:  {'simi': True, 'recur': 1, 'num_CC': 2, 'num_tri': False}\n",
      "t3:  {'simi': True, 'recur': 0, 'num_CC': 2, 'num_tri': False}\n",
      "t4:  {'simi': False, 'recur': 0, 'num_CC': 1, 'num_tri': False}\n",
      "t5:  {'simi': True, 'recur': 0, 'num_CC': 2, 'num_tri': False}\n",
      "t6:  {'simi': False, 'recur': 0, 'num_CC': 1, 'num_tri': False}\n",
      "t7:  {'simi': False, 'recur': 0, 'num_CC': 0, 'num_tri': False}\n",
      "t8:  {'simi': False, 'recur': 0, 'num_CC': 3, 'num_tri': False}\n",
      "t9:  {'simi': True, 'recur': 0, 'num_CC': 0, 'num_tri': False}\n",
      "t10:  {'simi': True, 'recur': 0, 'num_CC': 1, 'num_tri': False}\n",
      "t11:  {'simi': False, 'recur': 0, 'num_CC': 1, 'num_tri': False}\n",
      "t12:  {'simi': False, 'recur': 0, 'num_CC': 0, 'num_tri': False}\n",
      "t13:  {'simi': False, 'recur': 0, 'num_CC': 1, 'num_tri': False}\n"
     ]
    }
   ],
   "source": [
    "def get_features_new2(sent):\n",
    "    features = {}\n",
    "    recurrence = 0\n",
    "    similar_phrase_pair = 0\n",
    "    reversed_simi_pair = 0\n",
    "    \n",
    "    norm_sent = re.sub(\"...?_CC (also|not)_RB\", \",_,\", sent)  # nomalization: change \"but_CC also_RB/not_RB\" this kind of pattern into \",_,\"\n",
    "    norm_sent = re.sub(\"are_VBP not_RB\", \",_,\", norm_sent)\n",
    "    norm_sent = re.sub(\",_,.?,_,\", \",_,\", norm_sent)         # last step could result with \",_, ,_,\"\n",
    "    norm_sent = re.sub(\",_, ...?_CC\", \",_,\", norm_sent)      # nomalization: change \",_, and_CC/or_CC/but_CC\" this kind of pattern into \",_,\"\n",
    "    coor_indexes = get_index_of_coor(norm_sent)\n",
    "    process_sent = norm_sent.split()[: -1]                   # chop the \"sentence closer off\"\n",
    "           \n",
    "    if len(coor_indexes) == 1:   # only 2 sub-sentences\n",
    "        prev, after = process_sent[: coor_indexes[0]], process_sent[coor_indexes[0] + 1 :]\n",
    "        similar_phrase_pair += similar(prev, after)\n",
    "        recurrence += recurrence_w(prev, after)\n",
    "        if \"nor\" in sent:\n",
    "            similar_phrase_pair += reversed_simi(prev, after)\n",
    "        \n",
    "    elif len(coor_indexes) > 1:\n",
    "        prev = process_sent[: coor_indexes[0]]             # First sentence\n",
    "        for ii in range(len(coor_indexes) - 1):\n",
    "            after = process_sent[coor_indexes[ii] + 1 : coor_indexes[ii + 1]]\n",
    "            similar_phrase_pair += similar(prev, after)\n",
    "            recurrence += recurrence_w(prev, after)\n",
    "            if \"nor\" in sent:\n",
    "                similar_phrase_pair += reversed_simi(prev, after)\n",
    "            prev = after\n",
    "            \n",
    "        after = process_sent[coor_indexes[-1] + 1 :]        # Last sentence\n",
    "        similar_phrase_pair += similar(prev, after)\n",
    "        recurrence += recurrence_w(prev, after)\n",
    "        if \"nor\" in sent:\n",
    "            similar_phrase_pair += reversed_simi(prev, after)\n",
    "            \n",
    "# If by now no similar structures are found, try to split the sentence by <comma>, because conjunction words\n",
    "# like \"and\" might be part of the parallel elements\n",
    "    s = sent[:-4]     # chop off the sentence-closer\n",
    "    ssplit = s.split(\",_,\")\n",
    "    if len(ssplit) > 1:\n",
    "        for i in range(len(ssplit)-1):\n",
    "            similar_phrase_pair += similar(ssplit[i].split(), ssplit[i+1].split())\n",
    "\n",
    "                \n",
    "# If by now no similar structures are found, try to identify [A, B cc C], [A cc B cc C] or [A, B, C] types (structure consist of only one word) \n",
    "    \n",
    "    if len(sent.split()) > 4:      # for these types, sentence should contain at least 5 words.\n",
    "        tag_sequence = [w[w.index(\"_\") :] for w in sent.split()]  # use full PoS-Tag\n",
    "        for index in range(len(tag_sequence) - 4):\n",
    "            t1 = tag_sequence[index]\n",
    "            t2 = tag_sequence[index + 2]\n",
    "            t3 = tag_sequence[index + 4]\n",
    "            if t1 == t2 == t3 and ((tag_sequence[index+1] == \"_CC\" and tag_sequence[index+3] == \"_CC\") or (\n",
    "            tag_sequence[index+1] == \"_,\" and (tag_sequence[index+3] == \"_,\" or tag_sequence[index+3] == \"_CC\"))):\n",
    "                #print(tag_sequence)\n",
    "                similar_phrase_pair += 1\n",
    "                \n",
    "# If still no similar structures are found, try to identify [A, B cc C] types (structure consist of only one word)                             \n",
    "   \n",
    "                    \n",
    "    features[\"simi\"] = similar_phrase_pair > 0\n",
    "    features[\"recur\"] = recurrence\n",
    "    features[\"num_CC\"] = num_of_CC(sent)\n",
    "    features[\"num_tri\"] = trigram_repetition(sent) > 0\n",
    "    return features\n",
    "\n",
    "\n",
    "    \n",
    "test_sent1 = \"In_IN the_DT year_NN of_IN America_NNP 's_POS birth_NN ,_, in_IN the_DT coldest_JJS of_IN months_NNS ,_, a_DT small_JJ band_NN of_IN patriots_NNS huddled_VBN by_IN dying_VBG campfires_NNS on_IN the_DT shores_NNS of_IN an_DT icy_NN river_NN ._.\"\n",
    "test_sent2 = \"And_CC each_DT day_NN brings_VBZ further_JJ evidence_NN that_IN the_DT ways_NNS we_PRP use_VBP energy_NN strengthen_VB our_PRP$ adversaries_NNS and_CC threaten_VB our_PRP$ planet_NN ._.\"\n",
    "test_sent3 = \"America_NNP has_VBZ never_RB been_VBN united_VBN by_IN blood_NN or_CC birth_NN or_CC soil_NN ._.\"  \n",
    "t4 = \"I_PRP thank_VBP President_NNP Bush_NNP for_IN his_PRP$ service_NN to_TO our_PRP$ Nation_NNP ,_, as_RB well_RB as_IN the_DT generosity_NN and_CC cooperation_NN he_PRP has_VBZ shown_VBN throughout_IN this_DT transition_NN ._.\"\n",
    "t5 = \" an_DT education_NN system_NN ,_, flush_NN with_IN cash_NN ,_, but_CC which_WDT leaves_VBZ our_PRP$ young_JJ and_CC beautiful_JJ students_NNS deprived_VBN of_IN all_DT knowledge_NN ;_:\"\n",
    "t6 = \"Our_PRP$ Nation_NN is_VBZ at_IN war_NN against_IN a_DT far-reaching_JJ network_NN of_IN violence_NN and_CC hatred_NN ._.\"\n",
    "t7 = \"The_DT peaceful_JJ transfer_NN of_IN authority_NN is_VBZ rare_JJ in_IN history_NN ,_, yet_RB common_JJ in_IN our_PRP$ country_NN ._.\"\n",
    "t8 = \"We_PRP 've_VBP defended_VBN other_JJ nations_NNS '_POS borders_NNS while_IN refusing_VBG to_TO defend_VB our_PRP$ own_JJ and_CC spent_VBD trillions_NNS and_CC trillions_NNS of_IN dollars_NNS overseas_RB while_IN America_NNP 's_POS infrastructure_NN has_VBZ fallen_VBN into_IN disrepair_NN and_CC decay_NN ._.\"\n",
    "t9 = \"Permission_NN ,_, community_NN ,_, curiosity_NN :_:\"\n",
    "t11 = \"It_PRP is_VBZ the_DT firefighter_NN 's_POS courage_NN to_TO storm_VB a_DT stairway_NN filled_VBN with_IN smoke_NN ,_, but_CC also_RB a_DT parent_NN 's_POS willingness_NN to_TO nurture_VB a_DT child_NN ,_, that_WDT finally_RB decides_VBZ our_PRP$ fate_NN ._.\"\n",
    "t12 = \"Our_PRP$ spirits_NNS dampened_VBD ,_, we_PRP showed_VBD up_RP again_RB ,_, August_NNP 2018_CD ,_, year_NN 10_CD ._.\"\n",
    "t13 = \"We_PRP are_VBP shaped_VBN by_IN every_DT language_NN and_CC culture_NN ,_, drawn_VBN from_IN every_DT end_NN of_IN this_DT Earth_NNP ._.\"\n",
    "print(\"t1: \", get_features_new2(test_sent1))\n",
    "print(\"t2: \", get_features_new2(test_sent2))\n",
    "print(\"t3: \", get_features_new2(test_sent3))\n",
    "print(\"t4: \", get_features_new2(t4))\n",
    "print(\"t5: \", get_features_new2(t5))\n",
    "print(\"t6: \", get_features_new2(t6))\n",
    "print(\"t7: \", get_features_new2(t7))\n",
    "print(\"t8: \", get_features_new2(t8))\n",
    "print(\"t9: \", get_features_new2(t9))\n",
    "print(\"t10: \", get_features_new2(t10))\n",
    "print(\"t11: \", get_features_new2(t11))\n",
    "print(\"t12: \", get_features_new2(t12))\n",
    "print(\"t13: \", get_features_new2(t13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define this function for evaluation\n",
    "def compute_PRF(gold, predicted, class_label):\n",
    "    TP = sum(int(g == class_label and p == class_label) for (g, p) in zip (gold, predicted))\n",
    "    FP = sum(int(p == class_label and g != class_label) for (g, p) in zip (gold, predicted)) \n",
    "    FN = sum(int(p != class_label and g == class_label) for (g, p) in zip (gold, predicted))\n",
    "    if TP + FP > 0:\n",
    "        precision = TP/(TP + FP)\n",
    "    else:\n",
    "        precision = 0\n",
    "    if TP + FN > 0:\n",
    "        recall = TP/(TP + FN)\n",
    "    else:\n",
    "        recall = 0\n",
    "    if precision > 0 and recall > 0:\n",
    "        f_measure = 2 * precision * recall / (precision + recall)\n",
    "    else:\n",
    "        f_measure = 0\n",
    "    #print('Precision=%.2f Recall=%.2f F_Measure=%.2f'  %  (precision, recall, f_measure))\n",
    "    return (precision, recall, f_measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Classifiers from nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "380\n",
      "[('Inaugural_JJ Address_NNP January_NNP 20_CD ,_, 2009_CD Public_NNP Papers_NNP of_IN the_DT Presidents_NNS Barack_NNP Obama_NNP <_JJR br_NN >_JJR 2009_CD :_:\\n', 'f'), (' Book_VB I_PRP Barack_NNP Obama_NNP 2009_CD :_:\\n', 'f'), (' Book_VB I_PRP Location_NNP :_:\\n', 'f'), (' District_NNP of_IN Columbia_NNP Washington_NNP The_NNP American_NNP Presidency_NNP Project_NNP\\n', 'f'), ('My_PRP$ fellow_JJ citizens_NNS ,_, I_PRP stand_VBP here_RB today_NN humbled_VBN by_IN the_DT task_NN before_IN us_PRP ,_, grateful_JJ for_IN the_DT trust_NN you_PRP have_VBP bestowed_VBN ,_, mindful_JJ of_IN the_DT sacrifices_NNS borne_VBN by_IN our_PRP$ ancestors_NNS ._.', 't')]\n"
     ]
    }
   ],
   "source": [
    "data_set_raw1 = []\n",
    "for file in os.listdir(\"American-Inaugural-Address-Corpus/Tagged\"):\n",
    "    if file.endswith(\"csv\"):\n",
    "        with open(\"American-Inaugural-Address-Corpus/Tagged/\" + file, encoding = \"utf-8\") as f:\n",
    "            reader = csv.reader(f, delimiter = \",\")\n",
    "            rows = [row for row in reader]\n",
    "            data_set_raw1.extend(rows[1:])   # the first row is the header [\"sentence\", \"Tag\"], so not needed\n",
    "print(len(data_set_raw1))\n",
    "data_set = [(sent, tag) for [sent, tag] in data_set_raw1]\n",
    "print(data_set[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cross validated precision for 't' is:  0.8505120885555668\n",
      "\n",
      "cross validated recall for 't' is:  0.8435003450655625\n",
      "\n",
      "cross validated f-measure for 't' is:  0.8407891156462585\n"
     ]
    }
   ],
   "source": [
    "# cross validation\n",
    "k = 5\n",
    "sum_precision = 0\n",
    "sum_recall = 0\n",
    "sum_f_measure = 0\n",
    "random.Random(6).shuffle(data_set) \n",
    "featuresets = [(get_features_new2(sent), tag) for (sent, tag) in data_set ]\n",
    "size = len(featuresets)\n",
    "for fold in range(k):\n",
    "    train_set = featuresets[: int(size/k*fold)] + featuresets[int(size/k*(fold+1)):]\n",
    "    devtest_set = featuresets[int(size/k*fold) : int(size/k*(fold+1))]\n",
    "    nb = nltk.NaiveBayesClassifier.train(train_set)\n",
    "    gold = [tag for (sent_feature, tag) in devtest_set]\n",
    "    pred1 = [nb.classify(sent_feature) for (sent_feature, tag) in devtest_set]\n",
    "    sum_precision += compute_PRF(gold, pred1, \"t\")[0]\n",
    "    sum_recall += compute_PRF(gold, pred1, \"t\")[1]\n",
    "    sum_f_measure += compute_PRF(gold, pred1, \"t\")[2]\n",
    "\n",
    "print(\"\\ncross validated precision for 't' is: \", sum_precision/5)\n",
    "print(\"\\ncross validated recall for 't' is: \", sum_recall/5)\n",
    "print(\"\\ncross validated f-measure for 't' is: \", sum_f_measure/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                    simi = True                t : f      =     14.7 : 1.0\n",
      "                   recur = 2                   t : f      =     14.4 : 1.0\n",
      "                   recur = 1                   t : f      =     11.2 : 1.0\n",
      "                 num_tri = True                t : f      =      9.5 : 1.0\n",
      "                  num_CC = 0                   f : t      =      6.7 : 1.0\n"
     ]
    }
   ],
   "source": [
    "nb.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t'"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.classify(get_features_new2(\" that_IN we_PRP did_VBD not_RB turn_VB back_RB ,_, nor_CC did_VBD we_PRP falter_VBP ._.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guided_VBN by_IN these_DT principles_NNS once_RB more_RBR ,_, we_PRP can_MD meet_VB those_DT new_JJ threats_NNS that_WDT demand_VBP even_RB greater_JJR effort_NN ,_, even_RB greater_JJR cooperation_NN and_CC understanding_NN between_IN nations_NNS ._.\n",
      "\n",
      "For_IN we_PRP know_VBP that_IN our_PRP$ patchwork_NN heritage_NN is_VBZ a_DT strength_NN ,_, not_RB a_DT weakness_NN ._.\n",
      "\n",
      "We_PRP have_VBP a_DT place_NN ,_, all_DT of_IN us_PRP ,_, in_IN a_DT long_JJ story_NN ,_, a_DT story_NN we_PRP continue_VBP but_CC whose_WP$ end_NN we_PRP will_MD not_RB see_VB ._.\n",
      "\n",
      "For_IN many_JJ decades_NNS ,_, we_PRP 've_VBP enriched_VBN foreign_JJ industry_NN at_IN the_DT expense_NN of_IN American_JJ industry_NN ,_, subsidized_VBD the_DT armies_NNS of_IN other_JJ countries_NNS while_IN allowing_VBG for_IN the_DT very_RB sad_JJ depletion_NN of_IN our_PRP$ military_NN ._.\n",
      "\n",
      "that_WDT in_IN the_DT depth_NN of_IN winter_NN ,_, when_WRB nothing_NN but_CC hope_NN and_CC virtue_NN could_MD survive_VB ._.\n",
      "\n",
      "Yet_CC ,_, compassion_NN is_VBZ the_DT work_NN of_IN a_DT nation_NN ,_, not_RB just_RB a_DT government_NN ._.\n",
      "\n",
      "And_CC we_PRP find_VBP that_DT children_NNS and_CC community_NN are_VBP the_DT commitments_NNS that_WDT set_VBP us_PRP free_JJ ._.\n",
      "\n",
      "Online_NN by_IN Gerhard_NNP Peters_NNP and_CC John_NNP T._NNP Woolley_NNP ,_, The_NNP American_NNP Presidency_NNP Project_NNP ._.\n",
      "\n",
      "On_IN this_DT day_NN ,_, we_PRP gather_VBP because_IN we_PRP have_VBP chosen_VBN hope_NN over_IN fear_NN ,_, unity_NN of_IN purpose_NN over_IN conflict_NN and_CC discord_NN ._.\n",
      "\n",
      " an_DT education_NN system_NN ,_, flush_NN with_IN cash_NN ,_, but_CC which_WDT leaves_VBZ our_PRP$ young_JJ and_CC beautiful_JJ students_NNS deprived_VBN of_IN all_DT knowledge_NN ;_:\n",
      "\n",
      "They_PRP saw_VBD America_NNP as_IN bigger_JJR than_IN the_DT sum_NN of_IN our_PRP$ individual_JJ ambitions_NNS ,_, greater_JJR than_IN all_PDT the_DT differences_NNS of_IN birth_NN or_CC wealth_NN or_CC faction_NN ._.\n",
      "\n",
      "Online_NN by_IN Gerhard_NNP Peters_NNP and_CC John_NNP T._NNP Woolley_NNP ,_, The_NNP American_NNP Presidency_NNP Project_NNP ._.\n",
      "\n",
      "And_CC to_TO those_DT nations_NNS like_IN ours_JJ that_IN enjoy_VBP relative_JJ plenty_NN ,_, we_PRP say_VBP we_PRP can_MD no_RB longer_RB afford_VB indifference_NN to_TO suffering_VBG outside_IN our_PRP$ borders_NNS ,_, nor_CC can_MD we_PRP consume_VB the_DT world_NN 's_POS resources_NNS without_IN regard_NN to_TO effect_NN ,_, for_IN the_DT world_NN has_VBZ changed_VBN ,_, and_CC we_PRP must_MD change_VB with_IN it_PRP ._.\n",
      "\n",
      "The_DT establishment_NN protected_VBD itself_PRP ,_, but_CC not_RB the_DT citizens_NNS of_IN our_PRP$ country_NN ._.\n",
      "\n",
      "Online_NN by_IN Gerhard_NNP Peters_NNP and_CC John_NNP T._NNP Woolley_NNP ,_, The_NNP American_NNP Presidency_NNP Project_NNP ._.\n",
      "\n",
      " We_PRP are_VBP protected_VBN ,_, and_CC we_PRP will_MD always_RB be_VB protected_VBN ._.\n",
      "\n",
      "Together_RB ,_, we_PRP will_MD determine_VB the_DT course_NN of_IN America_NNP and_CC the_DT world_NN for_IN many_JJ ,_, many_JJ years_NNS to_TO come_VB ._.\n",
      "\n",
      "With_IN hope_NN and_CC virtue_NN ,_, let_VB us_PRP brave_VB once_RB more_JJR the_DT icy_NN currents_NNS and_CC endure_VB what_WP storms_NNS may_MD come_VB ._.\n",
      "\n",
      "We_PRP will_MD seek_VB friendship_NN and_CC good_NN will_MD with_IN the_DT nations_NNS of_IN the_DT world_NN ,_, but_CC we_PRP do_VBP so_RB with_IN the_DT understanding_NN that_IN it_PRP is_VBZ the_DT right_NN of_IN all_DT nations_NNS to_TO put_VB their_PRP$ own_JJ interests_NNS first_RB ._.\n",
      "\n",
      "God_NNP bless_VB you_PRP ,_, and_CC God_NNP bless_VBP the_DT United_NNP States_NNPS of_IN America_NNP ._.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for (sent, tag) in data_set:\n",
    "    if nb.classify(get_features_new2(sent)) == 't' and tag == 'f':\n",
    "        print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We_PRP are_VBP shaped_VBN by_IN every_DT language_NN and_CC culture_NN ,_, drawn_VBN from_IN every_DT end_NN of_IN this_DT Earth_NNP ._.\n",
      "\n",
      "The_DT ambitions_NNS of_IN some_DT Americans_NNS are_VBP limited_VBN by_IN failing_VBG schools_NNS and_CC hidden_VBN prejudice_NN and_CC the_DT circumstances_NNS of_IN their_PRP$ birth_NN ._.\n",
      "\n",
      "To_TO those_DT leaders_NNS around_IN the_DT globe_NN who_WP seek_VBP to_TO sow_VB conflict_NN or_CC blame_VB their_PRP$ society_NN 's_POS ills_NNS on_IN the_DT West_NNP ,_, know_VBP that_IN your_PRP$ people_NNS will_MD judge_VB you_PRP on_IN what_WP you_PRP can_MD build_VB ,_, not_RB what_WP you_PRP destroy_VBP ._.\n",
      "\n",
      "And_CC for_IN those_DT who_WP seek_VBP to_TO advance_VB their_PRP$ aims_NNS by_IN inducing_VBG terror_NN and_CC slaughtering_VBG innocents_NNS ,_, we_PRP say_VBP to_TO you_PRP now_RB that_IN our_PRP$ spirit_NN is_VBZ stronger_JJR and_CC can_MD not_RB be_VB broken_VBN ._.\n",
      "\n",
      "Duties_NNS that_IN we_PRP do_VBP not_RB grudgingly_RB accept_VB but_CC ,_, rather_RB ,_, seize_VB gladly_RB ,_, firm_NN in_IN the_DT knowledge_NN that_IN there_EX is_VBZ nothing_NN so_IN satisfying_VBG to_TO the_DT spirit_NN ,_, so_RB defining_VBG of_IN our_PRP$ character_NN ,_, than_IN giving_VBG our_PRP$ all_DT to_TO a_DT difficult_JJ task_NN ._.\n",
      "\n",
      "Its_PRP$ power_NN to_TO generate_VB wealth_NN and_CC expand_VB freedom_NN is_VBZ unmatched_JJ ._.\n",
      "We_PRP will_MD build_VB the_DT roads_NNS and_CC bridges_NNS ,_, the_DT electric_JJ grids_NNS and_CC digital_JJ lines_NNS that_WDT feed_VBP our_PRP$ commerce_NN and_CC bind_VBP us_PRP together_RB ._.\n",
      "\n",
      "My_PRP$ fellow_JJ citizens_NNS ,_, I_PRP stand_VBP here_RB today_NN humbled_VBN by_IN the_DT task_NN before_IN us_PRP ,_, grateful_JJ for_IN the_DT trust_NN you_PRP have_VBP bestowed_VBN ,_, mindful_JJ of_IN the_DT sacrifices_NNS borne_VBN by_IN our_PRP$ ancestors_NNS ._.\n",
      "This_DT is_VBZ the_DT price_NN and_CC the_DT promise_NN of_IN citizenship_NN ._.\n",
      "\n",
      "God_NNP bless_VB you_PRP all_DT ,_, and_CC God_NNP bless_VBP America_NNP ._.\n",
      "\n",
      "The_DT grandest_JJS of_IN these_DT ideals_NNS is_VBZ an_DT unfolding_VBG American_JJ promise_NN that_WDT everyone_NN belongs_VBZ ,_, that_IN everyone_NN deserves_VBZ a_DT chance_NN ,_, that_IN no_DT insignificant_JJ person_NN was_VBD ever_RB born_VBN ._.\n",
      "\n",
      "The_DT peaceful_JJ transfer_NN of_IN authority_NN is_VBZ rare_JJ in_IN history_NN ,_, yet_RB common_JJ in_IN our_PRP$ country_NN ._.\n",
      "\n",
      "Homes_NNPS have_VBP been_VBN lost_VBN ,_, jobs_NNS shed_VBD ,_, businesses_NNS shuttered_VBD ._.\n",
      "\n",
      "To_TO the_DT people_NNS of_IN poor_JJ nations_NNS ,_, we_PRP pledge_NN to_TO work_VB alongside_IN you_PRP to_TO make_VB your_PRP$ farms_NNS flourish_VB and_CC let_VB clean_JJ waters_NNS flow_NN ,_, to_TO nourish_VB starved_VBN bodies_NNS and_CC feed_NN hungry_JJ minds_NNS ._.\n",
      "\n",
      "''_'' America_NNP ,_, in_IN the_DT face_NN of_IN our_PRP$ common_JJ dangers_NNS ,_, in_IN this_DT winter_NN of_IN our_PRP$ hardship_NN ,_, let_VB us_PRP remember_VB these_DT timeless_JJ words_NNS ._.\n",
      "\n",
      "And_CC all_DT of_IN us_PRP are_VBP diminished_VBN when_WRB any_DT are_VBP hopeless_JJ ._.\n",
      "\n",
      "At_IN these_DT moments_NNS ,_, America_NNP has_VBZ carried_VBN on_IN not_RB simply_RB because_IN of_IN the_DT skill_NN or_CC vision_NN of_IN those_DT in_IN high_JJ office_NN ,_, but_CC because_IN we_PRP the_DT people_NNS have_VBP remained_VBN faithful_JJ to_TO the_DT ideals_NNS of_IN our_PRP$ forebears_NNS and_CC true_JJ to_TO our_PRP$ founding_VBG documents_NNS ._.\n",
      "\n",
      "And_CC though_IN it_PRP requires_VBZ sacrifice_NN ,_, it_PRP brings_VBZ a_DT deeper_JJR fulfillment_NN ._.\n",
      "\n",
      "On_IN this_DT day_NN ,_, we_PRP come_VBP to_TO proclaim_VB an_DT end_NN to_TO the_DT petty_NN grievances_NNS and_CC false_JJ promises_NNS ,_, the_DT recriminations_NNS and_CC worn-out_JJ dogmas_NNS that_WDT for_IN far_RB too_RB long_RB have_VB strangled_VBN our_PRP$ politics_NNS ._.\n",
      "\n",
      "Recall_VB that_DT earlier_JJR generations_NNS faced_VBD down_RP fascism_NN and_CC communism_NN not_RB just_RB with_IN missiles_NNS and_CC tanks_NNS but_CC with_IN sturdy_JJ alliances_NNS and_CC enduring_VBG convictions_NNS ._.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for (sent, tag) in data_set:\n",
    "    if nb.classify(get_features_new2(sent)) == 'f' and tag == 't':\n",
    "        print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t'"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.classify(get_features_new2(\"While_IN the_DT Congress_NNP determines_VBZ the_DT objects_NNS and_CC the_DT sum_NN of_IN appropriations_NNS ,_, the_DT officials_NNS of_IN the_DT executive_NN departments_NNS are_VBP responsible_JJ for_IN honest_JJ and_CC faithful_JJ disbursement_NN ,_, and_CC it_PRP should_MD be_VB their_PRP$ constant_JJ care_NN to_TO avoid_VB waste_NN and_CC extravagance_NN ._.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Classifiers from scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Inaugural_JJ Address_NNP January_NNP 20_CD ,_, 2009_CD Public_NNP Papers_NNP of_IN the_DT Presidents_NNS Barack_NNP Obama_NNP <_JJR br_NN >_JJR 2009_CD :_:\\n', 'f'], [' Book_VB I_PRP Barack_NNP Obama_NNP 2009_CD :_:\\n', 'f'], [' Book_VB I_PRP Location_NNP :_:\\n', 'f']]\n"
     ]
    }
   ],
   "source": [
    "data_set_raw = []\n",
    "for file in os.listdir(\"American-Inaugural-Address-Corpus/Tagged\"):\n",
    "    if file.endswith(\"csv\"):\n",
    "        with open(\"American-Inaugural-Address-Corpus/Tagged/\" + file, encoding = \"utf-8\") as f:\n",
    "            reader = csv.reader(f, delimiter = \",\")\n",
    "            rows = [row for row in reader]\n",
    "            data_set_raw.extend(rows[1:])\n",
    "print(data_set_raw[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "380"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_data = []\n",
    "for [sent, tag] in data_set_raw:\n",
    "    features_tag = []\n",
    "    simi = get_features_new2(sent)[\"simi\"]\n",
    "    recur = get_features_new2(sent)['recur']\n",
    "    num_CC = get_features_new2(sent)[\"num_CC\"]\n",
    "    num_tri = get_features_new2(sent)[\"num_tri\"]\n",
    "    features_tag.extend([sent,simi,recur,num_CC, num_tri,tag])\n",
    "    transformed_data.append(features_tag)  # use append to maintain list form\n",
    "len(transformed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent</th>\n",
       "      <th>simi</th>\n",
       "      <th>recur</th>\n",
       "      <th>num_CC</th>\n",
       "      <th>num_tri</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Inaugural_JJ Address_NNP January_NNP 20_CD ,_,...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Book_VB I_PRP Barack_NNP Obama_NNP 2009_CD :_:\\n</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Book_VB I_PRP Location_NNP :_:\\n</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>District_NNP of_IN Columbia_NNP Washington_NN...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My_PRP$ fellow_JJ citizens_NNS ,_, I_PRP stand...</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I_PRP thank_VBP President_NNP Bush_NNP for_IN ...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Forty-four_CD Americans_NNPS have_VBP now_RB t...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The_DT words_NNS have_VBP been_VBN spoken_VBN ...</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Yet_CC every_DT so_RB often_RB ,_, the_DT oath...</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>At_IN these_DT moments_NNS ,_, America_NNP has...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sent   simi  recur  num_CC  \\\n",
       "0  Inaugural_JJ Address_NNP January_NNP 20_CD ,_,...  False      0       0   \n",
       "1   Book_VB I_PRP Barack_NNP Obama_NNP 2009_CD :_:\\n  False      0       0   \n",
       "2                   Book_VB I_PRP Location_NNP :_:\\n  False      0       0   \n",
       "3   District_NNP of_IN Columbia_NNP Washington_NN...  False      0       0   \n",
       "4  My_PRP$ fellow_JJ citizens_NNS ,_, I_PRP stand...  False      1       0   \n",
       "5  I_PRP thank_VBP President_NNP Bush_NNP for_IN ...  False      0       1   \n",
       "6  Forty-four_CD Americans_NNPS have_VBP now_RB t...  False      0       0   \n",
       "7  The_DT words_NNS have_VBP been_VBN spoken_VBN ...   True      1       1   \n",
       "8  Yet_CC every_DT so_RB often_RB ,_, the_DT oath...   True      0       2   \n",
       "9  At_IN these_DT moments_NNS ,_, America_NNP has...  False      0       3   \n",
       "\n",
       "   num_tri tag  \n",
       "0    False   f  \n",
       "1    False   f  \n",
       "2    False   f  \n",
       "3    False   f  \n",
       "4    False   t  \n",
       "5    False   f  \n",
       "6    False   f  \n",
       "7    False   t  \n",
       "8    False   t  \n",
       "9    False   t  "
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data = pd.DataFrame(transformed_data, columns = ['sent','simi','recur',\"num_CC\",'num_tri','tag'])\n",
    "df_data.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data[\"tag_num\"] = df_data.tag.map({'f' : 0, 't' : 1})\n",
    "df_data.to_excel(\"American-Inaugural-Address-Corpus/Tagged/transformed.xlsx\", \n",
    "                 encoding = \"utf-8\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['simi','recur','num_CC','num_tri']\n",
    "# select all 10 features (X)\n",
    "X = df_data[feature_cols]\n",
    "# select numerical tag as responses/targets (y)\n",
    "y = df_data.tag_num "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use cross validation to evaluate the results:\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score, cross_val_predict\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different algorithms\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize classifiers\n",
    "knn = KNeighborsClassifier()\n",
    "lr = LogisticRegression()\n",
    "svc = LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "kf_5 = KFold(n = 380, n_folds = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tryClassifier(clf):\n",
    "    precisions = 0\n",
    "    recalls = 0\n",
    "    f_measures = 0\n",
    "\n",
    "    for (train_index, test_index) in kf_5:\n",
    "        X_train, X_test = np.asarray(X)[train_index], np.asarray(X)[test_index]  # convert dataFrame(X,y) into array!\n",
    "        y_train, y_test = np.asarray(y)[train_index], np.asarray(y)[test_index]  # Because index works with array!\n",
    "        \n",
    "        clf.fit(X_train, y_train)\n",
    "        tag_pred = clf.predict(X_test)\n",
    "\n",
    "        precisions += metrics.precision_score(y_test, tag_pred, 1) # cross-validated precision for label \"t\"\n",
    "        recalls += metrics.recall_score(y_test, tag_pred, 1)\n",
    "        f_measures += metrics.f1_score(y_test, tag_pred, 1)\n",
    "    print(type(clf))\n",
    "    print(\"CV_Precion for label 't':   \", precisions / 5)\n",
    "    print(\"CV_Recall for label 't':    \", recalls / 5)\n",
    "    print(\"CV_F_measure for label 't': \", f_measures / 5)\n",
    "\n",
    "    f1_scores = cross_val_score(clf, X, y, cv=5, scoring='f1')\n",
    "    \n",
    "    print(\"f1_scores_all_classes: \", f1_scores)\n",
    "    print(\"Average_f1_all_classed: \", sum([s for s in f1_scores])/5, \"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.neighbors.classification.KNeighborsClassifier'>\n",
      "CV_Precion for label 't':    0.8997902097902098\n",
      "CV_Recall for label 't':     0.7875979737141261\n",
      "CV_F_measure for label 't':  0.8327123202001252\n",
      "f1_scores_all_classes:  [0.77272727 0.68085106 0.90196078 0.93333333 0.88888889]\n",
      "Average_f1_all_classed:  0.8355522686186017 \n",
      "\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "CV_Precion for label 't':    0.8996935817805383\n",
      "CV_Recall for label 't':     0.8182644941265631\n",
      "CV_F_measure for label 't':  0.8511416177501481\n",
      "f1_scores_all_classes:  [0.77272727 0.72       0.94117647 0.93333333 0.93333333]\n",
      "Average_f1_all_classed:  0.860114081996435 \n",
      "\n",
      "<class 'sklearn.svm.classes.LinearSVC'>\n",
      "CV_Precion for label 't':    0.8986910755148741\n",
      "CV_Recall for label 't':     0.8177718832891246\n",
      "CV_F_measure for label 't':  0.8490266346086119\n",
      "f1_scores_all_classes:  [0.77272727 0.74509804 0.94117647 0.93333333 0.93333333]\n",
      "Average_f1_all_classed:  0.8651336898395723 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tryClassifier(knn)\n",
    "tryClassifier(lr)\n",
    "tryClassifier(svc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
